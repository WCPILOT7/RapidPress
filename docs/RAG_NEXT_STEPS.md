# RAG: Следующий этап

## Цель
Перейти от изолированного семантического поиска к Retrieval-Augmented Generation (RAG) при создании / правке пресс-релизов и сопутствующих материалов.

## Текущая база (готово)
- Ingestion: чанкинг + embeddings (JSON) + хранение в `ai_documents`
- Semantic Search: `/api/rag/search?mode=semantic` (cosine ranking)
- Keyword fallback: базовый эвристический поиск

## Основные пробелы
| Область | Статус | Комментарий |
|---------|--------|-------------|
| Векторное хранилище | TODO | Нужен pgvector + ANN индекс для scaling |
| Контекстная генерация | TODO | Генерация пока без retrieved контекста |
| Фильтрация источников | PARTIAL | Нет query-параметров source / parentId |
| Ограничение токенов | TODO | Нет обрезки по совокупной длине чанков |
| Кеширование запросов | TODO | Возможен memoization recent embeddings |

## План внедрения RAG-Augmented Generation
1. pgvector миграция
   - Добавить расширение `vector`
   - Добавить колонку `embedding_vector vector(1536)` (или размер текущей модели)
   - Backfill: пройтись по записям с JSON embedding → конвертировать → обновить
   - Индекс: `ivfflat` или `hnsw` (при поддержке) с подходящим lists
2. Retrieval слой
   - Модуль `server/ai/rag/retrieveContext.ts`: accepts (userId, query, opts) → возвращает top-k чанков (score, snippet)
   - Схема обрезки: суммарно ≤ N токенов (конфигurable)
   - Dedup по parentId
3. Интеграция в генерацию
   - Добавить вариант эндпоинта `/api/generate?mode=rag`
   - Если mode=rag: выполнять semantic search по: company name, ключевым словам из формы, предыдущим релизам
   - Prompt injection: вставить секцию CONTEXT:
     ```
     Используй ТОЛЬКО приведённый контекст где релевантно. Если факта нет в контексте — не выдумывай.
     ---
     {context_chunks}
     ---
     ```
4. Безопасность и контроль
   - Лимит чанков per запрос (например k=6)
   - Tracing: логировать сколько чанков и суммарные символы
   - Фича-флаг `ENABLE_RAG_MODE` (env)
5. Тесты
   - Юнит: retrieveContext (mock embeddings) → проверка сортировки
   - Интеграция: generate (mode=rag) возвращает текст, содержащий факт из контекста
6. Оптимизации (после базового запуска)
   - Кеширование query embedding на 5–10 минут (LRU)
   - Pre-warm embeddings для часто используемых документов (company boilerplate)
   - Hybrid ranking (BM25 + cosine) — позже

## Минимальные критерии готовности (Definition of Done)
- В prod конфиге можно включить RAG без редактирования кода (env флаг)
- `/api/generate?mode=rag` использует ≥1 контекстный чанк и отражает его содержимое
- Метрики фиксируют: кол-во чанков, суммарные символы, latency retrieval
- Документация обновлена (`ARCHITECTURE.md`, этот файл, PROGRESS_LOG)

## Риски
| Риск | Влияние | Смягчение |
|------|---------|-----------|
| Перегрузка токенов (слишком длинный контекст) | Удорожание, обрезка смысла | Жёсткий лимит + приоритизация чанков |
| Нерелевантный контекст | Снижение качества ответа | Скоринг + фильтр порогового score |
| Дубликаты / шум | Снижение плотности сигналов | Dedup parentId + TF-IDF нормализация (позже) |

## Следующие после этого шага
- Adaptive reranking (LLM judge небольшой prompt)
- Expansion: поддержка внешних источников (PDF ingest pipeline)
- Guardrails: фактическая проверка цитат (consistency pass)

---
Обновить при старте реализации и пометить даты завершения подэтапов.
